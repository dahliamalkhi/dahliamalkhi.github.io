---
title: 'Block-STM: Accelerating Smart-Contract Processing'
date: 2022-04-13
permalink: /posts/2022/04/block-stm/
tags:
  - blockchain
  - parallel execution
  - transactions
---

![](/images/bstm-scene.png)

[Block-STM](https://arxiv.org/pdf/2203.06871.pdf) is a recently announced technology for accelerating smart-contract execution, emanating from the 
[Diem project](https://github.com/diem)
and matured by 
[Aptos Labs](https://github.com/aptos-labs).
The acceleration approach interoperates with existing blockchains without requiring modification or adoption by miner/validator nodes, and can benefit any node independently when it validates transactions.

This post explains Block-STM in simple English accompanied with a running scenario.

## Background

An approach pioneered in the [Calvin 2012](http://cs.yale.edu/homes/thomson/publications/calvin-sigmod12.pdf) and [Bohm 2014](https://arxiv.org/pdf/1412.2324.pdf) projects in the context of distributed databases is the foundation of much of what follows. The insightful idea in those projects is to simplify concurrency management by disseminating pre-ordered batches (akin to blocks) of transactions along with pre-estimates of their read- and write- sets. 
Every database partition can then autonomously execute transactions according to the block pre-order, each transaction
waiting only for read dependencies on earlier transactions in the block. The [first DiemVM parallel executor](https://github.com/diem/diem/issues/8829) implements this approach but it relies on a static transaction analyzer to pre-estimate read/write-sets which is time consuming and can be inexact. 

Another work by [Dickerson et al. 2017](https://arxiv.org/abs/1702.04467)
provides a link from traditional database concurrency to smart-contract parallelism. In that work, a consensus *leader* (or *miner*) pre-computes a parallel execution serialization by
harnessing optimistic software transactional memory (STM) 
and disseminates the pre-execution scheduling guidelines to all *validator* nodes. 
A later work [OptSmart 2021](https://arxiv.org/abs/2102.04875) adds read/write-set dependency tracking during pre-execution and disseminates this information to increase parallelism. 
Those approaches remove the reliance on static transaction analysis but require a leader to pre-execute blocks.

The Block-STM parallel executor combines the pre-ordered block idea with optimistic STM to enforce the block pre-order of transactions on-the-fly, completely removing the need to pre-disseminate an execution schedule or pre-compute transaction dependencies, while guaranteeing repeatability.

## Technical Overview

Block-STM is a parallel execution engine for smart contracts, built around the principles of Software Transactional Memory. 
Transactions are grouped in blocks, each block containing a pre-ordered sequence of transactions
TX<sub>1</sub>, TX<sub>2</sub>, ..., TX<sub>n</sub>. Transactions consist of smart-contract code that reads and writes to shared memory and their
execution results in a read-set and a write-set: the read-set consists of pairs, a memory location and the transaction that wrote it; the write-set consists of pairs, a memory location and a value, that the transaction would record if it became committed.

**Block pre-order:**
A parallel execution of the block must yield the same deterministic outcome 
that preserves a block pre-order, namely, it results in exactly the same read/write sets as a sequential execution. 
If, in a sequential execution, TX<sub>k</sub> reads a value that TX<sub>j</sub> wrote, 
i.e., TX<sub>j</sub> is the highest transaction preceding TX<sub>k</sub> that writes to this particular memory location, 
we denote this by:
* TX<sub>j</sub> &rarr; TX<sub>k</sub> 

A parallel execution must guarantee that all transactions indeed read values adhering to these dependencies. 
That is, when TX<sub>k</sub> reads from memory, it must obtain the value(s) written by TX<sub>j</sub>, TX<sub>j</sub> &rarr; TX<sub>k</sub>, if a dependency exists;
or the initial value at that memory location when the block execution started, if none. 

**Correctness:**
Block-STM uses an optimistic approach, executing transactions greedily and optimistically in parallel and then validating.
Validation of TX<sub>k</sub> re-reads the read-set of TX<sub>k</sub> and compares against the original read-set that TX<sub>k</sub> obtained in its latest execution. If the comparison fails, TX<sub>k</sub> aborts and re-executes. 
Correct optimism revolves around maintaining two principles:

* **VALIDAFTER(j, k)**: For every j,k, such that j < k, a validation of TX<sub>k</sub> is performed after TX<sub>j</sub> executes (or re-executes).
* **READLAST(k)**: Whenever TX<sub>k</sub> executes (speculatively), a read by TX<sub>k</sub> obtains the value recorded so far by the highest transaction TX<sub>j</sub> preceding it, i.e., where j < k. Higher transactions TX<sub>l</sub>, where l > k, do not interfere with TX<sub>k</sub>. 

Jointly, these two principles 
suffice to guarantee both safety and liveness no matter what scheduling policy is used, so long as pending execution and validation tasks are eventually dispatched. Safety follows because a TX<sub>k</sub> gets validated after all TX<sub>j</sub>, j &lt; k, are finalized. Liveness follows by induction. Initially transaction 1 is guaranteed to pass validation successfully and not require re-execution. After all transactions from TX<sub>1</sub> to TX<sub>j</sub> have successfully validated, a (re-)execution of transaction j+1 will pass validation successfully and not require re-execution.

**READLAST(k)** is achieved via a simple multi-version in-memory data structure that keeps versioned write-sets. A write by TX<sub>j</sub> is recorded with version j. 
A read by TX<sub>k</sub> obtains the value recorded by the latest invocation of TX<sub>j</sub> with the highest j &lt; k.

A special value `ABORTED` may be stored at version j when the latest invocation of TX<sub>j</sub> aborts. 
If TX<sub>k</sub> reads this value, it suspends and resumes when the value becomes set.  

**VALIDAFTER(j, k)** is implemented by a scheduler. For each j, after TX<sub>j</sub> completes a (re-)execution, the scheduler dispatched every TX<sub>k</sub> with index k > j for (re)validation. 

## Scheduling

It remains to focus on devising an efficient schedule for parallelizing execution and validations. 

**Running example.** The following scenario will be used throughout this post to illustrate the effects of scheduling:

<pre style="font-size: 14px;">
    A block B consisting of ten transactions, 
    TX<sub>1</sub>, TX<sub>2</sub>, ..., TX<sub>10</sub>, with the following read/write dependencies:       
      
    TX<sub>1</sub> &rarr; TX<sub>2</sub> &rarr; TX<sub>3</sub> &rarr; TX<sub>4</sub>                
    TX<sub>3</sub> &rarr; TX<sub>6</sub>      
    TX<sub>3</sub> &rarr; TX<sub>9</sub>
</pre>

To illustrate execution timelines, we will illustrate scheduling the running example on four threads running on parallel cores and assume
each transaction takes exactly one time-unit, validations take negligible time. 

If we knew the block dependencies in advance, we could schedule 
an ideal execution of block B along the following time-steps:

<pre style="font-size: 14px;">
    1. parallel execution of TX<sub>1</sub>, TX<sub>5</sub>, TX<sub>6</sub>, TX<sub>7</sub>     
    2. parallel execution of TX<sub>2</sub>, TX<sub>8</sub>, TX<sub>10</sub>     
    3. parallel execution of TX<sub>3</sub>     
    4. parallel execution of TX<sub>4</sub>, TX<sub>6</sub>, TX<sub>9</sub>
</pre>

We now construct an effective scheduling strategy without knowing the dependencies in advance. We present the construction gradually, starting with a correct but inefficient strawman and gradually improving it in three steps. The full scheduling strategy is 
described in under 20 lines of pseudo-code. 

At a first cut, consider a strawman scheduler, S-1, that uses a centralized dispatcher that coordinates work by parallel threads.

## **S-1**

<pre style="font-size: 14px;">
    // Phase 1: 
    dispatch all TX<sub></sub>’s for execution in parallel ; wait for completion

    // Phase 2: 
    repeat {
        dispatch all TX<sub></sub>'s for validation in parallel ; wait for completion
    } until all validations pass

    validation of TX<sub>j</sub> {
        re-read TX<sub>j</sub> read-set 
        if read-set differs from original read-set of the latest TX<sub>j</sub> execution 
            re-execute TX<sub>j</sub>
    }

    execution of TX<sub>j</sub> {
        (re-)execute TX<sub>j</sub>
    }
</pre>

S-1 operates in two master-coordinated phases. Phase 1 executes all transactions optimistically in parallel. Phase 2 repeatedly validates all transactions optimistically in parallel, re-executing those that fail, until there are no more validation failures. 

With four threads, a possible execution of S-1 over Block B (recall, TX<sub>1</sub> &rarr; TX<sub>2</sub> &rarr; TX<sub>3</sub> &rarr; {TX<sub>4</sub>, TX<sub>6</sub>, TX<sub>9</sub>}) 
would proceed along the following time-steps:

<pre style="font-size: 14px;">
    1. phase 1 starts. parallel execution of TX<sub>1</sub>, TX<sub>2</sub>, TX<sub>3</sub>, TX<sub>4</sub>    
    2. parallel execution of TX<sub>5</sub>, TX<sub>6</sub>, TX<sub>7</sub>, TX<sub>8</sub>    
    3. parallel execution of TX<sub>9</sub>, TX<sub>10</sub>    
    4. phase 2 starts. parallel validation of all transactions in which TX<sub>2</sub>, TX<sub>3</sub>, TX<sub>4</sub>, TX<sub>6</sub> fail and re-execute    
    5. continued parallel validation of all transactions in which TX<sub>9</sub> fails and re-executes    
    6. parallel validation of all transactions in which TX<sub>3</sub>, TX<sub>4</sub> fail and re-execute    
    7. parallel validation of all transactions in which TX<sub>4</sub> fails and re-executes    
    8. parallel validation of all transactions in which all succeed
</pre>

It is quite easy to see that the S-1 validation loop satisfies VALIDAFTER(j,k) because every transaction is validated after previous executions complete.  However, it is quite wasteful in resources, each loop fully executing/validating all transactions.

The first improvement is to replace both phases with parallel task-*stealing* by threads. Using insight from S-1, we distinguish between a preliminary execution (corresponding to phase 1) and re-execution (following a validation abort).  Stealing is coordinated
via two synchronization counters, one per task type, `nextPrelimExecution` (initially 1) and `nextValidation` (initially 2). Each synchronizer `x` supports atomic procedures `x.increment() { oldVal := x; increment x; return oldVal } `and `x.setMin(val) { x := min(val, x) }. `

The following strawman scheduler, S-2, utilizes a task stealing regime:

## **S-2**

<pre style="font-size: 14px;">
    // per thread main loop: 
    repeat {

        // if available, steal next validation task
        if nextValidation < nextPrelimExecution
            j := nextValidation.increment() ; if j < nextPrelimExecution, validate TX<sub>j</sub>

        // if available, steal next execution task
        otherwise if nextPrelimExecution <= n
            j := nextPrelimExecution.increment() ; if j <= n, execute TX<sub>j</sub>

    } until nextPrelimExecution > n, nextValidation > n, and no task is still running

    validation of TX<sub>j</sub> {
        re-read TX<sub>j</sub> read-set 
        if read-set differs from original read-set of the latest TX<sub>j</sub> execution 
            re-execute TX<sub>j</sub>
    }

    execution of TX<sub>j</sub> {
        (re-)execute TX<sub>j</sub>
        nextValidation.setMin(j+1) 
    }
</pre>

Interleaving preliminary executions with validations avoids unnecessary work executing transactions that might follow aborted transactions. 
To illustrate this, we will once again utilize our running example.
The timing of task stealing over our running example is hard to predict because it depends on real-time latency and interleaving of validation and execution tasks. Notwithstanding, below is a possible execution of S-2 over B with four threads that exhibits
fewer (re-)executions and lower overall latency than S-1.

With four threads, a possible execution of S-2 over Block B (recall, TX<sub>1</sub> &rarr; TX<sub>2</sub> &rarr; TX<sub>3</sub> &rarr; {TX<sub>4</sub>, TX<sub>6</sub>, TX<sub>9</sub>}) 
would proceed along the following time-steps:

<pre style="font-size: 14px;">
    1. parallel execution of TX<sub>1</sub>, TX<sub>2</sub>, TX<sub>3</sub>, TX<sub>4</sub>; validation of TX<sub>2</sub>, TX<sub>3</sub>, TX<sub>4</sub> fail; nextValidation set to 3      
    2. parallel execution of TX<sub>2</sub>, TX<sub>3</sub>, TX<sub>4</sub>, TX<sub>5</sub>; validation of TX<sub>3</sub>, TX<sub>4</sub> fail; nextValidation set to 4      
    3. parallel execution of TX<sub>3</sub>, TX<sub>4</sub>, TX<sub>6</sub>, TX<sub>7</sub>; validation of TX<sub>4</sub>, TX<sub>6</sub> fail; nextValidation set to 5      
    4. parallel execution of TX<sub>4</sub>, TX<sub>6</sub>, TX<sub>8</sub>, TX<sub>9</sub>; all validations succeed     
    5. parallel execution of TX<sub>10</sub>; all validations succeed
</pre>

Importantly, **VALIDAFTER(j, k)** is preserved because upon (re-)execution of a TX<sub>j</sub>, it decreases `nextValidation` to j+1. This guarantees that every k > j will be validated after the j execution. 

Preserving **READLAST(k)** requires care due to concurrent task stealing, since multiple *incarnations* of the same transaction validation or execution tasks may occur simultaneously. Recall that **READLAST(k)** requires that a read by a TX<sub>k</sub> should obtain the value recorded by the latest invocation of a TX<sub>j</sub> with the highest j &lt; k. This requires to synchronize transaction invocations, such that **READLAST(k)** returns the highest incarnation value recorded by a transaction. A simple solution is to use per-transaction atomic incarnation synchronizer that prevents stale incarnations from recording values.

The last improvement step consists of two important improvements.

The first is an extremely simple dependency tracking (no graphs or partial orders) that considerably reduces aborts. When a TX<sub>j</sub> aborts, the write-set of its latest invocation is marked `ABORTED`. READLAST(k) supports the `ABORTED` mark guaranteeing that a higher-index TX<sub>k</sub> reading from a location in this write-set will delay until the TX<sub>j</sub> completes re-executing.

The second one increases re-validation parallelism. When a transaction aborts, rather than waiting for it to complete re-execution, it decreases `nextValidation` immediately; then, if the re-execution writes to a (new) location which is not marked `ABORTED`, `nextValidation` is decreased again when the re-execution completes. 

The final scheduling algorithm S-3, has the same main loop body at S-2 with executions 
supporting dependency managements via `ABORTED` tagging, and with early re-validation enabled by decreasing `nextValidation` upon abort:

## **S-3**

<pre style="font-size: 14px;">
    // per thread main loop, same as S-2
    ...

    validation of TX<sub>j</sub>:
    {
        re-read TX<sub>j</sub> read-set 
        if read-set differs from original read-set of the latest TX<sub>j</sub> execution 
            mark the TX<sub>j</sub> write-set ABORTED
            nextValidation.setMin(j+1) 
            re-execute TX<sub>j</sub>
    }

    execution of TX<sub>j</sub>:  
    {
        (re-)execute TX<sub>j</sub>
        if the TX<sub>j</sub> write-set contains locations not marked ABORTED
            nextValidation.setMin(j+1) 
    }
</pre>

S-3 enhances efficiency through simple, on-the-fly dependency management using the `ABORTED` tag. For our running example of block B, 
An execution driven by S-3 with four threads may be able to avoid several of the re-executions incurred in S-2 by waiting on an ABORTED mark. 
Despite the high-contention B scenario, a possible execution of S-3 may achieve very close to optimal scheduling as shown below.
A possible execution of S-3 over Block B (recall, TX<sub>1</sub> &rarr; TX<sub>2</sub> &rarr; TX<sub>3</sub> &rarr; {TX<sub>4</sub>, TX<sub>6</sub>, TX<sub>9</sub>}) 
would proceed along the following time-steps:

<pre style="font-size: 14px;">
    1. parallel execution of TX<sub>1</sub>, TX<sub>2</sub>, TX<sub>3</sub>, TX<sub>4</sub>; validation of TX<sub>2</sub>, TX<sub>3</sub>, TX<sub>4</sub> fail; nextValidation set to 3      
    2. parallel execution of TX<sub>2</sub>, TX<sub>5</sub>, TX<sub>7</sub>, TX<sub>8</sub>; executions of TX<sub>3</sub>, TX<sub>4</sub>, TX<sub>6</sub> are suspended on ABORTED; nextValidation set to 6    
    3. parallel execution of TX<sub>3</sub>, TX<sub>10</sub>; executions of TX<sub>4</sub>, TX<sub>6</sub>, TX<sub>9</sub> are suspended on ABORTED; all validations succeed (for now)    
    4. parallel execution of TX<sub>4</sub>, TX<sub>6</sub>, TX<sub>9</sub>; all validations succeed
</pre>

The reason S-3 preserves **VALIDAFTER(j, k)** is slightly subtle. Suppose that TX<sub>j</sub> &rarr; TX<sub>k</sub>.
Recall, when TX<sub>j</sub> fails, S-3 lets (re-)validations of TX<sub>k</sub>, k > j, proceed before TX<sub>j</sub> completes re-execution. There are two possible cases. If a TX<sub>k</sub>-validation reads an `ABORTED` value of TX<sub>j</sub>, it will wait for TX<sub>j</sub> to complete; and if it reads a value which is not marked `ABORTED` and the TX<sub>j</sub> re-execution overwrites it, then TX<sub>k</sub> will be forced to revalidate again.

*Disclaimer: The description above reflects more-or-less faithfully the Block-STM approach; for details, see the [paper](https://arxiv.org/pdf/2203.06871.pdf) (note, the description above uses different names from the paper, e.g., `ABORTED` replaces “ESTIMATE”, `nextPrelimExecution` replaces “execution_idx”, `nextValidation` replaces “validation_idx”).*

## Conclusion

Through a careful combination of simple, known techniques and applying them to a pre-ordered block of transactions that commit at bulk, 
Block-STM enables effective speedup of smart contract parallel processing. Simplicity is a virtue of Block-STM, not a failing, enabling a robust and stable implementation. 
Block-STM has been integrated within the Diem blockchain core ([https://github.com/diem/](https://github.com/diem/)) and evaluated on synthetic transaction workloads, yielding over 17x speedup on 32 cores under low/modest contention. 

